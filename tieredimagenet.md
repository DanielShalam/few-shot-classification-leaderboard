---
title: tieredImageNet
layout: leaderboard
---


## *tiered*ImageNet Leaderboard (5-class)

#### [Edit this leaderboard](https://github.com/yaoyao-liu/few-shot-classification-leaderboard/edit/main/tieredimagenet.md)

Method   | Venue | Year | Backbone   | Setting | 1-shot      | 5-shot   | Code | Reported by 
------- | ------ | ---- | --------   | -----    | -----   | -----    | ---- | ----
[MetaOptNet](https://arxiv.org/pdf/1904.03758.pdf)     | CVPR   | 2019 | ResNet-12  | Inductive |  65.99 ± 0.72    | 81.56 ± 0.53     | [\[PyTorch\]](https://github.com/kjunelee/MetaOptNet) | [\[Source\]](https://arxiv.org/pdf/1904.03758.pdf)
[LEO](https://arxiv.org/pdf/1807.05960.pdf) | ICLR | 2019 | WRN-28-10 | Inductive | 66.33 ± 0.05 | 82.06 ± 0.08 | [\[TensorFlow\]](https://github.com/deepmind/leo) | [\[Source\]](https://arxiv.org/pdf/1807.05960.pdf)
[ProtoNets](https://arxiv.org/pdf/1703.05175.pdf) | NeurIPS | 2017 | 4CONV | Inductive | 53.31 ± 0.89 | 72.69 ± 0.74 | [\[PyTorch\]](https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch) | [\[Source\]](https://arxiv.org/pdf/1703.05175.pdf)
[RelationNets](https://arxiv.org/pdf/1711.06025.pdf) | CVPR | 2018 | 4CONV | Inductive | 54.48 ± 0.93 | 71.32 ± 0.78  | [\[PyTorch\]](https://github.com/floodsung/LearningToCompare_FSL) | [\[Source\]](https://arxiv.org/pdf/1904.03758.pdf)
[TPN](https://arxiv.org/pdf/1805.10002.pdf) | ICLR | 2019 | 4CONV | Transductive | 59.91 ± 0.94 |  73.30 ± 0.75  | [\[TensorFlow\]](https://github.com/csyanbin/TPN) | [\[Source\]](https://arxiv.org/pdf/1904.03758.pdf)
[DeepEMD](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf) | CVPR | 2020 | ResNet-12 | Inductive |  71.16 ± 0.87 | 86.03 ± 0.58  | [\[PyTorch\]](https://github.com/icoz69/DeepEMD) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
[ProtoNets](https://arxiv.org/pdf/1703.05175.pdf) | NeurIPS | 2017 | ResNet-12 | Inductive | 65.65 ± 0.92 | 83.40. ± 0.65 | [\[PyTorch\]](https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
[MatchingNets](https://arxiv.org/pdf/1606.04080.pdf) | NeurIPS | 2016 | ResNet-12 | Inductive | 68.50 ± 0.92 | 80.60 ± 0.71  | [\[TensorFlow\]](https://github.com/AntreasAntoniou/MatchingNetworks) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
[CTM](https://arxiv.org/pdf/1905.11116.pdf) | CVPR | 2019 | ResNet-18 | Inductive | 68.41 ± 0.39 | 84.28 ± 1.73 | [\[PyTorch\]](https://github.com/Clarifai/few-shot-ctm) | [\[Source\]](https://arxiv.org/pdf/1905.11116.pdf)
[wDAE-GNN](https://arxiv.org/pdf/1905.01102.pdf) | CVPR | 2019 | WRN-28-10 | Inductive | 68.18 ± 0.16 | 83.09 ± 0.12  | [\[PyTorch\]](https://github.com/gidariss/wDAE_GNN_FewShot) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
[PPA](https://arxiv.org/pdf/1706.03466.pdf) | CVPR | 2018 | WRN-28-10 | Inductive | 65.65 ± 0.92 | 83.40. ± 0.65  | None | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_DeepEMD_Few-Shot_Image_Classification_With_Differentiable_Earth_Movers_Distance_and_CVPR_2020_paper.pdf)
[CAN](https://papers.nips.cc/paper/8655-cross-attention-network-for-few-shot-classification.pdf) | NeurIPS | 2019 | ResNet-12 | Inductive |  69.89 ± 0.51 | 84.23 ± 0.37 | [\[PyTorch\]](https://github.com/blue-blue272/fewshot-CAN) | [\[Source\]](https://papers.nips.cc/paper/8655-cross-attention-network-for-few-shot-classification.pdf)
[CAN+T](https://papers.nips.cc/paper/8655-cross-attention-network-for-few-shot-classification.pdf) | NeurIPS | 2019 | ResNet-12 | Transductive |  73.21 ± 0.58 | 84.93 ± 0.38  | [\[PyTorch\]](https://github.com/blue-blue272/fewshot-CAN) | [\[Source\]](https://papers.nips.cc/paper/8655-cross-attention-network-for-few-shot-classification.pdf)
[FEAT](https://arxiv.org/pdf/1812.03664.pdf) | CVPR | 2020 | WRN-28-10 | Inductive |  70.41 ± 0.23 | 84.38 ± 0.16 | [\[PyTorch\]](https://github.com/Sha-Lab/FEAT) | [\[Source\]](https://arxiv.org/pdf/1812.03664.pdf)
[FEAT](https://arxiv.org/pdf/1812.03664.pdf) | CVPR | 2020 | ResNet-12 | Inductive |  70.80 ± 0.23  | 84.79 ± 0.16 | [\[PyTorch\]](https://github.com/Sha-Lab/FEAT) | [\[Source\]](https://arxiv.org/pdf/1812.03664.pdf)
[Dhillon et al.](https://openreview.net/pdf?id=rylXBkrYDS) | ICLR | 2020 | WRN-28-10 | Transductive |  73.34 ± 0.71 | 85.50 ± 0.50 | None | [\[Source\]](https://openreview.net/pdf?id=rylXBkrYDS)
[SIB](https://openreview.net/pdf?id=Hkg-xgrYvH) | ICLR | 2020 | WRN-28-10 | Transductive |  72.9 | 82.8 | [\[PyTorch\]](https://github.com/hushell/sib_meta_learn) | [\[Source\]](https://openreview.net/pdf?id=Hkg-xgrYvH)
[Ravichandran et al.](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf) | ICCV | 2019 | 4CONV | Inductive | 48.19 ± 0.43 | 65.50 ± 0.39 | None| [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf)
[Ravichandran et al.](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf) | ICCV | 2019 | ResNet-12 | Inductive | 66.87  | 82.64 | None| [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf)
[Robust 20-dist++](https://openaccess.thecvf.com/content_ICCV_2019/papers/Dvornik_Diversity_With_Cooperation_Ensemble_Methods_for_Few-Shot_Classification_ICCV_2019_paper.pdf) | ICCV | 2019 | ResNet-12 | Inductive |70.44 ± 0.32  | 85.43 ± 0.21 | [\[PyTorch\]](https://github.com/dvornikita/fewshot_ensemble) | [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Dvornik_Diversity_With_Cooperation_Ensemble_Methods_for_Few-Shot_Classification_ICCV_2019_paper.pdf)
[CC+rot](https://openaccess.thecvf.com/content_ICCV_2019/papers/Gidaris_Boosting_Few-Shot_Visual_Learning_With_Self-Supervision_ICCV_2019_paper.pdf) | ICCV | 2019 | WRN-28-10 | Inductive | 62.93 ± 0.45 | 79.87 ± 0.33 | [\[PyTorch\]](https://github.com/valeoai/BF3S) | [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Gidaris_Boosting_Few-Shot_Visual_Learning_With_Self-Supervision_ICCV_2019_paper.pdf)
[DeepEMD v2](https://arxiv.org/pdf/2003.06777.pdf) | arXiv | 2020 | ResNet-12 | Inductive |  74.29 ± 0.32 | 87.08 ± 0.60  | [\[PyTorch\]](https://github.com/icoz69/DeepEMD) | [\[Source\]](https://arxiv.org/pdf/2003.06777.pdf)
[E<sup>3</sup>BM](https://arxiv.org/pdf/1904.08479.pdf) | ECCV | 2020 | ResNet-12 | Inductive |  71.2 ± 0.4 | 85.3 ± 0.3 | [\[PyTorch\]](https://gitlab.mpi-klsb.mpg.de/yaoyaoliu/e3bm) | [\[Source\]](https://arxiv.org/pdf/1904.08479.pdf)
[E<sup>3</sup>BM](https://arxiv.org/pdf/1904.08479.pdf) | ECCV | 2020 | WRN-28-10 | Transductive |  75.6 ± 0.6 | 84.3 ± 0.4  | [\[PyTorch\]](https://gitlab.mpi-klsb.mpg.de/yaoyaoliu/e3bm) | [\[Source\]](https://arxiv.org/pdf/1904.08479.pdf)
[LST](https://papers.nips.cc/paper/9216-learning-to-self-train-for-semi-supervised-few-shot-classification.pdf) | NeurIPS | 2019 | ResNet-12 | Semi-supervised |  77.7 ± 1.6 | 85.2 ± 0.8  | [\[TensorFlow\]](https://github.com/xinzheli1217/learning-to-self-train) | [\[Source\]](https://papers.nips.cc/paper/9216-learning-to-self-train-for-semi-supervised-few-shot-classification.pdf)
[AM3](https://papers.nips.cc/paper/8731-adaptive-cross-modal-few-shot-learning.pdf) | NeurIPS | 2019 | ResNet-12 | Inductive |  69.08 ± 0.47 | 82.58 ± 0.31  | [\[TensorFlow\]](https://github.com/ElementAI/am3) | [\[Source\]](https://papers.nips.cc/paper/8731-adaptive-cross-modal-few-shot-learning.pdf)
[DSN-MR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Simon_Adaptive_Subspaces_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 | ResNet-12 | Inductive |  67.39 ± 0.82 | 82.85 ± 0.56 | [\[PyTorch\]](https://github.com/chrysts/dsn_fewshot) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Simon_Adaptive_Subspaces_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
[DPGN](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 | ResNet-12 | Transductive |  72.45 ± 0.51 | 87.24 ± 0.39  | [\[PyTorch\]](https://github.com/megvii-research/DPGN) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
[DPGN](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 | 4CONV | Transductive |  69.43 ± 0.49 | 85.92 ± 0.42  | [\[PyTorch\]](https://github.com/megvii-research/DPGN) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
[LR+ICI](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Credibility_Inference_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 |  ResNet-12 | Transductive |  80.79 | 87.92 | [\[PyTorch\]](https://github.com/Yikai-Wang/ICI-FSL) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Credibility_Inference_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
[LR+ICI](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Credibility_Inference_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 |  ResNet-12 | Semi-supervised |  84.01 | 89.00 | [\[PyTorch\]](https://github.com/Yikai-Wang/ICI-FSL) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Credibility_Inference_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
[BD-CSPN](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460715.pdf) | ECCV | 2020 |  WRN-28-10 | Transductive |  78.74 ± 0.95 |  86.92 ± 0.63 | None | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460715.pdf)
[Centroid alignment](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500018.pdf) | ECCV | 2020 |  WRN-28-10 | Inductive |  74.40 ± 0.68 | 86.61 ± 0.59 | [\[PyTorch\]](https://github.com/ArmanAfrasiyabi/associative-alignment-fs) | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500018.pdf)
[ICA + MSP](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520511.pdf) | ECCV | 2020 | DenseNet | Transductive |   84.29 ± 0.25 | 89.31 ± 0.15 | None | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520511.pdf)
[ICA + MSP](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520511.pdf) | ECCV | 2020 | DenseNet | Semi-supervised |  86.00 ± 0.23 |  89.39 ± 0.15 | None | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520511.pdf)
[Y. Tian et al.](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590256.pdf) | ECCV | 2020 | ResNet-12 | Inductive | 71.52 ± 0.69 | 86.03 ± 0.49 | [\[PyTorch\]](https://github.com/WangYueFt/rfs) | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590256.pdf)
[EPNet](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710120.pdf) | ECCV | 2020 | WRN-28-10 | Transductive | 78.50 ± 0.91 |  88.36 ± 0.57  | [\[PyTorch\]](https://github.com/ElementAI/embedding-propagation) | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710120.pdf)
[EPNet](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710120.pdf) | ECCV | 2020 | WRN-28-10 | Semi-supervised | 83.69 ± 0.99  | 89.34 ± 0.59  | [\[PyTorch\]](https://github.com/ElementAI/embedding-propagation) | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710120.pdf)
[F. Wu et al.](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730239.pdf) | ECCV | 2020 | Capsule Network | Inductive | 69.87 ± 0.32  | 86.35 ± 0.41  | None | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730239.pdf)
[LR+ICI v2](https://arxiv.org/pdf/2007.08461.pdf) | arXiv | 2020 |  ResNet-12 | Semi-supervised |  84.44 ± 1.00  | 89.99 ± 0.55 | [\[PyTorch\]](https://github.com/Yikai-Wang/ICI-FSL) | [\[Source\]](https://arxiv.org/pdf/2007.08461.pdf)
[LR+ICI v2](https://arxiv.org/pdf/2007.08461.pdf) | arXiv | 2020 |  ResNet-12 | Transductive |  81.51 ± 1.07 | 88.48 ± 0.60 | [\[PyTorch\]](https://github.com/Yikai-Wang/ICI-FSL) | [\[Source\]](https://arxiv.org/pdf/2007.08461.pdf)
[Su et al.](https://arxiv.org/pdf/1910.03560.pdf) | ECCV | 2020 | ResNet-18 | Inductive | -  | 78.9 ± 0.7  | [PyTorch](https://github.com/cvl-umass/fsl_ssl) | [\[Source\]](https://arxiv.org/pdf/1910.03560.pdf)
[TIM](https://proceedings.neurips.cc/paper/2020/file/196f5641aa9dc87067da4ff90fd81e7b-Paper.pdf) | NeurIPS | 2020 | ResNet-18 | Transductive | 80.0 | 88.5  | [PyTorch](https://github.com/mboudiaf/TIM) | [\[Source\]](https://proceedings.neurips.cc/paper/2020/file/196f5641aa9dc87067da4ff90fd81e7b-Paper.pdf)
[TIM](https://proceedings.neurips.cc/paper/2020/file/196f5641aa9dc87067da4ff90fd81e7b-Paper.pdf) | NeurIPS | 2020 | WRN-28-10 | Transductive | 82.1  | 89.8  | [PyTorch](https://github.com/mboudiaf/TIM) | [\[Source\]](https://proceedings.neurips.cc/paper/2020/file/196f5641aa9dc87067da4ff90fd81e7b-Paper.pdf)
[IFSL](https://proceedings.neurips.cc/paper/2020/file/1cc8a8ea51cd0adddf5dab504a285915-Paper.pdf) | NeurIPS | 2020 | WRN-28-10 | Transductive | 83.07  | 88.69  | [PyTorch](https://github.com/yue-zhongqi/ifsl) | [\[Source\]](https://proceedings.neurips.cc/paper/2020/file/1cc8a8ea51cd0adddf5dab504a285915-Paper.pdf)
